Bayesian Regression Models with RStanARM
========================================================
author: TJ Mahr
date: Sept. 21, 2016
autosize: true
incremental: false

Madison R Users Group

<small>
[Github repository](https://github.com/tjmahr/MadR_RStanARM)
<br/>
[@tjmahr](https://twitter.com/tjmahr)
</small>

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.asp = 0.618,
  fig.width = 6,
  dpi = 300,
  fig.align = "center",
  out.width = "70%",
  comment = "#>",
  collapse = TRUE
)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)
```




Overview
===============================================================================

- [How I got into Bayesian statistics](http://rpubs.com/tjmahr/rep-crisis)
- [Some intuition-building about Bayes theorem](http://rpubs.com/tjmahr/bayes-theorem)
- **[Tour of RStanARM](http://rpubs.com/tjmahr/rstanarm-tour)**
- [Where to learn more about Bayesian statistics](http://rpubs.com/tjmahr/bayes-learn-more)





Tour of RStanARM
===============================================================================
type: section




Software ecosystem
===============================================================================
type: sub-section




What is Stan?
===============================================================================

A programming language for probablistic stats.

- Write out a description of the data and model using a mathy syntax.
- The model is _compiled_ into an executable that does the sampling.
- Pystan and RStan are interfaces that help you send/receive data to/from Stan
  models.




Simple Regression Model in Stan
===============================================================================

```{r, eval = FALSE}
# R version
lm(log(earnings) ~ height + male)
```

```
data {
  int<lower=0> N;
  vector[N] earn;
  vector[N] height;
  vector[N] male;
}
transformed data {
  // log transformation
  vector[N] log_earn;
  log_earn = log(earn);
}
parameters {
  vector[3] beta;
  real<lower=0> sigma;
}
model {
  log_earn ~ normal(beta[1] + beta[2] * height + beta[3] * male, sigma);
}
generated quantities {
  // optional
}
```




Take aways
===============================================================================

* Program is a description of the data and a model.
* Language wants a more formal information about the data (type, length,
  bounds), presumably for more efficient sampling.
* The tilde operator `~` is a sampling statement.




Another language?
===============================================================================

![Gif of Princess Leia saying "no thanks"](./assets/no-thanks.gif)




RStanARM
===============================================================================
type: sub-section




What is RStanArm?
===============================================================================

RStan Applied Regression Modeling

- Batteries-included, precompiled versions of common regression models.
- `glm` -> `stan_glm`, `glmer` -> `stan_glmer`.
- Write your regular code. Add `stan_` to the front, and add a prior.
- [CRAN page](https://cran.r-project.org/web/packages/rstanarm/)
  - Very good! They have lots of detailed vignettes!
- Proper successor to the `arm` package.




An example: Height and Weight by Sex
===============================================================================

```{r}
# Some toy data
davis <- car::Davis %>% filter(100 < height) %>% as_data_frame

davis
```




Classical model: Summary
===============================================================================

```{r, digits = 2}
model <- lm(weight ~ height * sex, davis)
summary(model) %>% print(digits = 2)
```




Classical model: Predicted mean over x
===============================================================================

```{r davis-classical, echo = FALSE, fig.cap = "Scatterplot of height/weight, colored by sex, with model-predicted mean for each group drawn."}
plot_limits <- davis %>%
  group_by(sex) %>%
  filter(height %in% range(height, na.rm = TRUE)) %>%
  ungroup

plot_limits$fitted <- predict(model, plot_limits)

p1 <- ggplot(davis) +
  aes(x = height, y = weight, color = sex) +
  geom_line(aes(y = fitted), data = plot_limits, size = 1.25) +
  geom_point() + theme(legend.position = c(0, 1), legend.justification = c(0, 1))
p1
```




Fitting the model with RStanARM
===============================================================================
type: sub-section




Load the package
===============================================================================

```{r}
library(rstanarm)
#> Loading required package: Rcpp
#> rstanarm (Version 2.12.1, packaged: 2016-09-12 13:08:24 UTC)
#> - Do not expect the default priors to remain the same in future rstanarm versions.
#> Thus, R scripts should specify priors explicitly, even if they are just the defaults.
#> - For execution on a local, multicore CPU with excess RAM we recommend calling
#> options(mc.cores = parallel::detectCores())
```

* So... hard-code the priors.
* I only set the `mc.cores` option when I have a big model that going to take
  more than a minute to fit.




Fit the model
===============================================================================
class: scrollable-slide
incremental: false

* We have to use `stan_glm()`.
  * `stan_lm()` uses a different specification of the prior.
* By default, it does sampling with 4 MCMC chains.
  * Each chain is 2000 samples, but the first half are warm-up samples.
  * Warm-up samples are ignored.

```{r stan-davis, cache = FALSE}
stan_model <- stan_glm(
  weight ~ height * sex,
  data = davis,
  family = gaussian,
  prior = normal(location = 0, scale = 5),
  prior_intercept = normal(0, 10)
)
# comment to make the bottom of the output visible
```




First look: Just printing the model object
===============================================================================
incremental: false

```{r}
stan_model
```




Getting a summary from the model
===============================================================================
class: scrollable-slide
incremental: false

```{r}
summary(stan_model)
# comment to make the bottom of the output visible
```




Notes on summary()
===============================================================================

* Split into estimation and diagnostic information
* `mean_PPD` is the predicted value for a completely average observation




Inspecting posterior samples
===============================================================================
type: sub-section




Looking at the posterior parameter samples
===============================================================================

Coerce to a data-frame. Columns are parameters. One row per posterior sample.

```{r}
samples <- stan_model %>% as.data.frame %>% tbl_df
samples
```




Looking at the posterior parameter samples
===============================================================================
title: false

```{r histogram-of-height-effect, fig.cap = "Histogram of height effect."}
ggplot(samples) + aes(x = height) + geom_histogram()
```




Quantiles are post-data probabilities
===============================================================================
class: scrollable-slide
incremental: false

- If we believe there is a "true" value for a parameter, there is 90%
  probability that this "true" value is in the 90% interval, given our model,
  prior information, and the data.
- The 90% interval contains the middle 90% of the parameter values.
  - There is a 5% chance, says the model, the height parameter is below the
    5% quantile.

```{r}
posterior_interval(stan_model)
```




Plotting the parameters
===============================================================================
incremental: false

Basic `plot()` method shows 80% and 95% intervals.

```{r basic-parameter-plot, fig.cap = "Basic plot() called on model. Shows 80%, 95% intervals for each parameter."}
plot(stan_model)
```




Plotting some of the parameters
===============================================================================
incremental: false

```{r subset-of-parameters, fig.cap = "Basic plot() called on model. Shows 80%, 95% intervals. Limited to just parameters matching 'height'."}
# Match a subset of the parameters
plot(stan_model, regex_pars = "height")
```




Exploring with ShinyStan
===============================================================================
incremental: false

This thing is pretty awesome.

```{r, eval = FALSE}
launch_shinystan(stan_model)
```

[_Demo outside of slides._]




Things I just demonstrated in ShinyStan
===============================================================================
incremental: false

- Diagnostics
  - Mixing of chains
  - R hat, n eff
  - PPcheck
- Parameters plot
  - Different parameters selected
  - KDE
- Explore
  - Inspect one parameter in multiview
  - Bivariate view




Plotting model results
===============================================================================
type: sub-section




Doing Things With The Model
===============================================================================

Get a data-frame with the parameters from each sample. We now have 4000 plausble regression lines.

```{r}
df_model <- stan_model %>% as_data_frame()
df_model
```



Doing Things With The Model
===============================================================================
title: false

Apply the group effects to get regression lines for each sex.

```{r}
df_model2 <- df_model %>%
  mutate(F_Intercept = `(Intercept)`, F_Slope = height,
         M_Intercept = `(Intercept)` + sexM,
         M_Slope = height + `height:sexM`) %>%
  select(F_Intercept:M_Slope)
df_model2
```

The "Pile of Lines" Plot
===============================================================================

Plot lines with the median parameter values and a random sample of lines.

```{r}
fits <- sample_n(df_model2, 200)
medians <- df_model2 %>% summarise_each(funs = funs(median))

p2 <- ggplot(davis) +
  aes(x = height, y = weight, color = sex) +
  geom_abline(aes(color = "F", intercept = F_Intercept,
                  slope = F_Slope), data = fits, alpha = .075) +
  geom_abline(aes(color = "M", intercept = M_Intercept,
                  slope = M_Slope), data = fits, alpha = .075) +
  geom_abline(aes(color = "F", intercept = F_Intercept,
                  slope = F_Slope), data = medians, size = 1.25) +
  geom_abline(aes(color = "M", intercept = M_Intercept,
                  slope = M_Slope), data = medians, size = 1.25) +
  geom_point() +
  theme(legend.position = c(0, 1), legend.justification = c(0, 1))
```




The "Pile of Lines" Plot
===============================================================================
title: false

```{r pile-of-lines, echo = FALSE, fig.cap = "Pile of lines plot. Scatter plot of height and weight. One thick line for each group's posterior median. 200 other lines, drawn from posterior, for each group also drawn."}
p2
```




"Pile of Lines"
===============================================================================

- Depicts uncertainty by drawing line of best fit plus many other plausible
  regression lines.
- Visualization works best when there is only one group and we don't mind having
  regression lines stretch past the range of the data.




The "Line + Prediction Interval"  Plot
===============================================================================

Get a sample of height values within each group's range.

```{r, echo = FALSE}
grid_range <- function(xs) {
  rng <- range(xs, na.rm = TRUE)
  seq(rng[1], rng[2], length.out = 80)
}

df_m <- davis %>%
  filter(sex == "M") %>%
  getElement("height") %>%
  grid_range %>%
  data_frame(sex = "M", height = .)

df_f <- davis %>%
  filter(sex == "F") %>%
  getElement("height") %>%
  grid_range %>%
  data_frame(sex = "F", height = .)

new_data <- bind_rows(df_f, df_m) %>% tibble::rownames_to_column("Observation")
```

```{r}
# # do some stuff
# ...

new_data
```




Get the predicted mean for each point
===============================================================================

With the normal `predict()` function on a classical regression model, we use the
model parameters to get the expected mean ($\mu_i$) for each row in `newdata`.

With `posterior_linpred()`, we do the same thing, but for each of the 4000
posterior samples.

```{r}
linpreds <- posterior_linpred(stan_model, newdata = new_data)
dim(linpreds)
```



Get the predicted mean for each point
===============================================================================
incremental: false
title: false

```{r}
# In long format... 4000 samples x 160 data points
tidy_linpreds <- linpreds %>%
  as_data_frame %>%
  tibble::rownames_to_column("Draw") %>%
  tidyr::gather(Observation, Value, -Draw)

tidy_linpreds
```


Get the regression fit (expected mean) for each point
===============================================================================
title: false

Summarize the posterior predictions for each new data point. Go from 4000 rows
per new point to just one row per new data point.

```{r}
df_predictions <- tidy_linpreds %>%
  group_by(Observation) %>%
  summarise(median = median(Value),
            ymin = quantile(Value, .025),
            ymax = quantile(Value, .975)) %>%
  left_join(new_data)

df_predictions
```


Now, we can plot predicted means
===============================================================================

```{r}
p3 <- ggplot(davis) +
  aes(x = height, y = weight, color = sex, group = sex) +
  geom_point() +
  geom_ribbon(aes(ymin = ymin, ymax = ymax, y = NULL, color = NULL),
              data = df_predictions, fill = "grey60", alpha = .4) +
  geom_line(aes(y = median), data = df_predictions, size = 1.25) +
  theme(legend.position = c(0, 1), legend.justification = c(0, 1))
```


Now, we can plot predicted means
===============================================================================
title: false

```{r posterior-linpred-plot, echo = FALSE, fig.cap = "Scatterplot of data with plot of predicted means and 95% credible intervals for each sex."}
p3
```



Limitations of this approach
===============================================================================

RStanARM does not like `posterior_linpred()`. Documentation says:

> **See also**: `posterior_predict` to draw from the posterior predictive
> distribution of the outcome, which is almost always preferable.

<br/>
Why? Because we have samples of the error term `sigma`, and we're not using
them!




Get the model to generate new observations
===============================================================================

Same plan as before, but use `posterior_predict()` to get predictions for
observations ($y_i$), instead of estimates of the mean ($\mu_i$).

```{r}
# ?posterior_predict
preds <- posterior_predict(stan_model, newdata = new_data)
dim(preds)
```


Get the model to generate new observations
===============================================================================
title: false
incremental: false

```{r}
tidy_preds <- preds %>%
  # Rename columns from V1, V2, ... to 1, 2, ...
  as_data_frame %>% setNames(seq_len(ncol(.))) %>%
  tibble::rownames_to_column("Draw") %>%
  tidyr::gather(Observation, Value, -Draw)

tidy_preds
```


Get the model to generate new observations
===============================================================================
title: false
incremental: false


```{r}
df_predictions <- tidy_preds %>%
  group_by(Observation) %>%
  summarise(median = median(Value),
            ymin = quantile(Value, .025),
            ymax = quantile(Value, .975)) %>%
  left_join(new_data)

df_predictions
```


Plot the predictions
===============================================================================
incremental: false

```{r}
p4 <- ggplot(davis) +
  aes(x = height, y = weight, color = sex, group = sex) +
  geom_point() +
  geom_ribbon(aes(ymin = ymin, ymax = ymax, y = NULL, color = NULL),
              data = df_predictions, fill = "grey60", alpha = .2) +
  geom_line(aes(y = median),
              data = df_predictions, size = 1.25) +
  theme(legend.position = c(0, 1), legend.justification = c(0, 1))
```



Plot the predictions
===============================================================================
title: false
incremental: false


```{r posterior-predict-plot, echo = FALSE, fig.cap = "Scatterplot of data with medians of simulated data and 95% posterior predictive intervals for each sex."}
p4
```




Model comparison
===============================================================================
type: sub-section




Model comparison
===============================================================================

Do we need the sex-height interaction?

Do we need the sex predictor at all?

Let's fit alternative models.

```{r alternatives, results = 'hide', cache = TRUE}
stan_model_no_inter <- update(stan_model, weight ~ sex + height)
stan_model_no_group <- update(stan_model, weight ~ height)
```


Classical model comparison
===============================================================================

For classical regression models, we could compare models with AIC and BIC.

```{r, cache = TRUE}
model_no_inter <- update(model, weight ~ sex + height)
model_no_group <- update(model, weight ~ height)
model_list <- list(
  no_group = model_no_group,
  no_inter = model_no_inter,
  inter = model)

model_list %>% lapply(AIC) %>% unlist

model_list %>% lapply(BIC) %>% unlist
```



Classical model comparison
===============================================================================

Or with `anova()`.

```{r}
anova(model_no_group, model_no_inter, model)
```


Approximate Leave One Out Cross Validation
===============================================================================

For RStanARM, we compare the models by using approximation leave-one-out
cross-validation. The details are outside of the scope of this talk.

```{r, cache = TRUE}
loo1 <- loo(stan_model_no_group)
loo2 <- loo(stan_model_no_inter)
loo3 <- loo(stan_model)
```




Approximate Leave One Out Cross Validation
===============================================================================
title: false

```{r}
compare(loo1, loo2, loo3)
```

- Like other IC, lower is better. Importantly, the LOOIC comes with a standard error.
- By point-value (`looic`), the model with the interaction is expected to perform the best on out-of-sample data.
- But the `se_looic` indicates considerable overlap with the model with no interaction term.
- Like so much of Bayesian modeling, uncertainty is everything.




Comparing the model to the priors
===============================================================================
type: sub-section




Priors
===============================================================================

- In our models, the prior `normal(0, 5)` gets rescaled to match each predictor
  variable.
- This rescaling is
  [not well documented](https://groups.google.com/forum/#!category-topic/stan-users/rstanarm/_y6aDbkdANw).
- Some options:
  - `scale()` our measurements so that they are
    [scale free](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
    (with mean = 0 and SD = 1).
  - Sample from the prior to see what kinds of values are generated by our
    prior and confirm that our prior information yields sensible model behavior.




Sampling from the prior: posterior_vs_prior()
===============================================================================
incremental: false

RStanARM will compare samples from the posterior and the prior.

```{r posterior-vs-prior-1, results = 'hide', fig.cap = "default posterior_vs_prior() plot. It looks bad because one parameter ruins y-axis."}
posterior_vs_prior(stan_model)
```




Sampling from the prior: posterior_vs_prior()
===============================================================================
title: false
incremental: false

```{r posterior-vs-prior-2, results = 'hide', fig.cap = "posterior_vs_prior() plot on just the 'height' parameters."}
# Select just parameters with "height" in the name
posterior_vs_prior(stan_model, regex_pars = "height")
```




Sampling from the prior: posterior_vs_prior()
===============================================================================
title: false
incremental: false
class: scrollable-slide

```{r posterior-vs-prior-3, cache = TRUE, results = 'hide', fig.cap = "posterior_vs_prior() plot with facets for each variable. This looks best because the y-axis varies freely by facet."}
# use the faceting options
comparison <- posterior_vs_prior(
  stan_model,
  group_by_parameter = TRUE,
  facet_args = list(scales = "free_y"),
  prob = .95)

comparison + theme_grey() + guides(color = FALSE)
```



Sampling from  the prior: prior_PD = TRUE
===============================================================================
incremental: false

We can also sample from the prior directly.

```{r, cache = TRUE, results = 'hide'}
stan_model_prior <- stan_glm(
  weight ~ height * sex,
  data = davis,
  family = gaussian,
  prior = normal(0, 5),
  prior_intercept = normal(0, 10),
  # this line is new:
  prior_PD = TRUE
)
```




Sampling the prior: prior_PD = TRUE
===============================================================================
title: false
incremental: false
class: scrollable-slide

Our initial prior says that an increase in height of 1 cm may predict a 0 +/-
7 kg increase in weight in women. This is a very generous interval for this
effect!

```{r}
summary(stan_model_prior, probs = c(.1, .5, .9))
# comment
```

This is an extra slide about stan_lm().
===============================================================================
class: scrollable-slide

We didn't use `stan_lm()` because those models have one prior for all the
parameters: _R_^2.

```{r, cache = TRUE, results = 'hide'}
lm_model <- stan_lm(
  weight ~ height * sex,
  data = davis,
  # Prior: I asked Wolfram Alpha for height and weight
  # correlation in adults and squared it
  prior = R2(0.63))
```




This is an extra slide about stan_lm().
===============================================================================
class: scrollable-slide
title: false

```{r}
summary(lm_model)
```




Finally
===============================================================================

- [How I got into Bayesian statistics](http://rpubs.com/tjmahr/rep-crisis)
- [Some intuition-building about Bayes theorem](http://rpubs.com/tjmahr/bayes-theorem)
- [Tour of RStanARM](http://rpubs.com/tjmahr/rstanarm-tour)
- **[Where to learn more about Bayesian statistics](http://rpubs.com/tjmahr/bayes-learn-more)**
